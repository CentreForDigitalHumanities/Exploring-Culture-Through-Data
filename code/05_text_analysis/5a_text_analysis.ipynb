{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to explore how to analyze large bodies of text using Natural Language Processing (NLP). With NLP, we can get a completely different perspectives on textual data that simply aren't possible using traditional close reading methods. \n",
    "\n",
    "We are going to work with the Python library *NLTK* (Natural Language Toolkit), which contains a variety of programs to work with data containing human language. Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bge_j\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bge_j\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bge_j\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk import *\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From text to tokens\n",
    "A computer does not *read* a text. To process a collection of texts on a computational level, we need to approach our text in a different way. \n",
    "\n",
    "Our first step will be to split our text up into individual tokens. Instead of full bodies of text, we are going to work with our texts on a different scale. By splitting up the texts into individual words (or sentences), we are able to run computations on the word frequencies. \n",
    "\n",
    "Let's start by tokenizing the sentence below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Everyday',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'reminded',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'koala',\n",
       " \"'s\",\n",
       " 'fingerprints',\n",
       " 'are',\n",
       " 'almost',\n",
       " 'identical',\n",
       " 'to',\n",
       " 'those',\n",
       " 'of',\n",
       " 'a',\n",
       " 'human',\n",
       " '.',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'cute',\n",
       " '!']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySentence = \"Everyday, I'm reminded of the fact that koala's fingerprints are almost identical to those of a human. That's cute!\"\n",
    "word_tokenize(mySentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split it on a different level. What's the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Everyday, I'm reminded of the fact that koala's fingerprints are almost identical to those of a human.\",\n",
       " \"That's cute!\"]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(mySentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is most common in NLP to work on a word level, as this allows for more fine-grained analyses. Let's stick to that.\n",
    "\n",
    "There are multiple ways of tackling tokenization:\n",
    "\n",
    "- **Bag-of-Words (BoW):** Counts the frequency of words in each document.\n",
    "- **Term Frequency-Inverse Document Frequency (TF-IDF):** Weighs the frequency of words by how common or rare they are across all documents.\n",
    "- **Word Embeddings:** Represent words in continuous vector space (e.g., Word2Vec, GloVe). This method considers the context of the words within the larger text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short explanation of  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short explanation of concept and practical application of tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo, wereld\n"
     ]
    }
   ],
   "source": [
    "# Show how to vectorize a corpus of cleaned texts into a tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
