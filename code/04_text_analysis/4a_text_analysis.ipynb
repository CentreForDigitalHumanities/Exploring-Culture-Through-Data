{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing in Python\n",
    "\n",
    "In this tutorial, we are going to explore how to analyze large bodies of text using Natural Language Processing (NLP). With NLP, we can get completely different perspectives on textual data that simply aren't possible using traditional close reading methods. \n",
    "\n",
    "We are going to work with a variety of Python library (mainly [``nltk`` (Natural Language Toolkit)](https://www.nltk.org/) and ``spacy``), which contain a variety of scripts to work with data containing human language. Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import *\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From text to tokens\n",
    "A computer does not read a text like a human would.  To process a collection of texts on a computational level, we need to approach our text in a different way. This is often called _distant reading_.\n",
    "\n",
    "Our first step will be to split our text up into individual tokens. Instead of full bodies of text, we are going to work with our texts on a different scale. By splitting up the texts into individual words (or sentences), we are able to run computations on the word frequencies. \n",
    "\n",
    "Let's start by tokenizing the sentence below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myText = \"It's summer time! Time to go outside! Well, first we have to finish today's workshops, of course. Take your time.\"\n",
    "word_tokenize(myText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's tokenize on a different level. What's the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(myText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is most common in NLP to work on a word level, as this allows for more fine-grained analyses. We'll stick to that for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a word frequency analysis is a basic way of getting to know the contents of a text. It shows you which words are most common across the document.\n",
    "\n",
    "Let's start by getting the the word frequency for a large text. Go to the website for **[Project Gutenberg](https://www.gutenberg.org/)** and download a book to your liking. Make sure that you download it in .txt format. Save the file in a convient location.\n",
    "\n",
    "> **Exercise:** Before importing your book, we need to make sure if the text file is cleaned. Open the text file in a text editor and clean it, so that only the contents of the book itself are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change the name and location to those of your own text file.\n",
    "with open(r'C:\\Users\\bge_j\\Documents\\UDS\\Summer school 2024\\Materials\\04_text_analysis\\novel.txt', 'r',  encoding='utf-8') as file:\n",
    "    myBook = file.read()\n",
    "    myBook = ' '.join(myBook.split())\n",
    "\n",
    "print(myBook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** There's a part in our script above that's a little tricky: `myBook = ' '.join(myBook.split())`. Try to deconstruct this line and note down what it does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported a clean text, let's see what the most common words in the text are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTokens = word_tokenize(myBook)\n",
    "wordFrequency = FreqDist(token.lower() for token in myTokens)\n",
    "\n",
    "wordFrequency.plot(25, title='Top 25 most frequent words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a start, but this is not very telling, is it? There's still a lot of noise in our tokens - interpunction, but especially words that are not informative (the, he, of, and, a, etc.). Let's clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# First, we lower our tokens. Then we filter out the ones in our stopwords list. Finally, we remove every token that is not a word.\n",
    "filteredTokens = [word for word in myTokens if word.lower() not in stopwords.words('english') and re.sub(r'[^\\w\\s]','',word)]\n",
    "\n",
    "# Plot the results. This code is the same as above, but now using our filteredTokens object.\n",
    "wordFrequency = FreqDist(token.lower() for token in filteredTokens)\n",
    "wordFrequency.plot(25, title='Top 25 most frequent words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks more like it! \n",
    "\n",
    "We used a collection of English stopwords from NLTK to remove words that are very common and thus not informative. We can see what's in this list by printing its contents. \n",
    "\n",
    "> **Exercise:** Print the contents of the stopwords list we used in the script above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to add more words to your stopwords list. For example, in my novel the tokens \"said\", \"would\" and \"'s\" are very common. We can add these to our list of stopwords.\n",
    "\n",
    "> **Exercise:** Add a few more words to the stopwords list and save the result to a new list called `myStopwords`. Create a new word frequency plot using your own stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a part of the above code that might look a little daunting at first. Let's isolate this part to see what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'[^\\w\\s]','','Hey dear reader... This is my sample text!!!! :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the function, all punctuation is removed from the text. But why? What does `r'[^\\w\\s]'` do? Time to learn about...\n",
    "\n",
    "#### Regular expressions\n",
    "This is what is known as a **regular expression** (also often called **regex**). Regex is a way of creating complex patterns to find parts of text using rules. Regex can be hard to read, but there are a lot of useful resources to learn its syntax. We recommend [RegExr](https://regexr.com/). This website allows you to paste your own text, write a regex pattern and immediately what parts of your text it matches with. It also includes a handy cheat sheet.\n",
    "\n",
    "Let's take the above regex expression as an example and deconstruct it: `r'[^\\w\\s]'`.\n",
    "\n",
    "- `r`: this isn't part of the regular expression, but it is important. By including **r** in front of a string, Python will parse the text *as is*. This includes backslashes `\\`, which normally serve a different function in a regex (long story, read more about that [here](https://docs.python.org/3/library/re.html)).\n",
    "- `[]`: a group of characters. For example, `[a-f]` matches all letters from a to f.\n",
    "- `^`: negation, meaning that the regex will match everything that is **not** stated in the set.\n",
    "- `\\w`: matches all words.\n",
    "- `\\s`: matches all whitespaces (spaces, newlines, tabs).\n",
    "\n",
    "Put together, this regex pattern matches all parts of the text that are **not** words or whitespaces. This is an effective way of cleaning all interpunction from our text.\n",
    "\n",
    "As you might see, regex aren't very intuitive at first. Luckily, many people have published their regex patterns online. If you are looking for a specific pattern, try searching on StackOverflow or using an AI assistant, such as ChatGPT. By learning how other build their patterns, you'll start to gain an understanding of how to create your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyword in context (concordance view)\n",
    "\n",
    "After running our word frequency analysis, we have a broad overview of the most important words in our text. The context is missing, however. To see how these words are used in the document we can use a concordance view, which shows a keyword in its original context. Try searching for keywords with the script below and see what you can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = Text(myTokens)\n",
    "\n",
    "# Find the concordance of a word\n",
    "nltk_text.concordance('me')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance view is useful for doing qualitative analysis. It allows you to get an overview of the different contexts in which a term is used within your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Document Frequency (tf-idf)\n",
    "\n",
    "Next, let's go beyond simple word counts. We are going to work with a more advanced metric: **tf-idf**. This metric builds on the foundation of the word frequency we calculated earlier, but places it in the context of a larger corpus of texts. tf-idf is used to calculate how common a word is in the target document compared to the rest of the corpus. The result is a score between 0 and 1 that describes the importance of the word for the given document; the higher the score, the more important the term is to the document. tf-idf is a common method in information retrieval, as it is an efficient way to get a broad overview of the contents of a text. \n",
    "\n",
    "> _An example of a practical application of tf-idf can be a study of news coverage about a particular news event, where you are interested in studying how different newspapers cover the same story. Let's say you want to study news coverage about the Summer Olympics. In this case, you could create a corpus that contains documents for each newspaper. These documents contain all articles that particular outlet published about the Olympic Games. Since each article is about the same news event, we can expect common words describing the event to score low on tf-idf. The high-scoring terms, on the other hand, reveal the subjects and themes that are important to each publication; these are the topics that they spend more coverage on compared to other newspapers._ \n",
    "\n",
    "For this next exercise, we're going to create our own corpus and calculate tf-idf scores for each document. \n",
    "\n",
    "> **Exercise:** Go back to **[Project Gutenberg](https://www.gutenberg.org/)** and download at least five books that you think make for an interesting comparison. Perhaps you want to compare the work of authors writing about the same subject, or books published in the same year. Write down why you want to compare the contents of these books. Again, make sure to download the files as .txt and clean them before moving on to the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your own folder containing the novels\n",
    "corpus_path = r'C:\\Users\\bge_j\\Documents\\UDS\\Summer school 2024\\Materials\\04_text_analysis\\corpus' \n",
    "\n",
    "# Initialize an empty list to store the contents of the files\n",
    "corpus = []\n",
    "\n",
    "# Iterate through the files in the folder and \n",
    "for filename in os.listdir(corpus_path):\n",
    "    file_path = os.path.join(corpus_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            corpus.append(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Explain what the code above does. What does the object ``corpus`` contain?\n",
    "\n",
    "Next, we're going to apply the tf-idf model to the corpus we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Calculate TF-IDF using scikit-learn\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words) and their corresponding TF-IDF scores\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print the words with the highest tf-idf score for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 10 words with the highest tf-idf scores for each document\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc = doc.replace(\"\\n\", \"\") # Remove newlines from the documents for readability in the printout\n",
    "    print(f\"------------- Document {i+1}: {doc[:30]} -------------\\n\")\n",
    "    \n",
    "    # Get the indices of the top 10 words with the highest TF-IDF scores\n",
    "    top_10_indices = np.argsort(tfidf_scores[i])[::-1][:10]\n",
    "    \n",
    "    # Print the top 10 words and their corresponding TF-IDF scores\n",
    "    for j in top_10_indices:\n",
    "        print(f\"{feature_names[j]}: {tfidf_scores[i][j]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Check the results from the tf-idf above. What does it tell you about your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition\n",
    "\n",
    "In addition to counting our text, we can classify parts of our text as well. A popular method is **Named Entity Recognition (NER)**, which uses a large language model to recognize textual structures and classify individual tokens as so-called 'named entities'.\n",
    "\n",
    "Let's start by importing the necessary libraries.\n",
    "- [**spaCy**](https://spacy.io/) is a popular library for working with textual data. It contains a number of prebuilt functions for doing advanced textual analysis, including NER.\n",
    "- [**en_core_web_sm**](https://spacy.io/models/en) is an English language model, mainly trained on web data (blogs, news articles, comments). It contains pipelines for parts-of-speech tagging, amongst other functions. In the following scripts, we're going to apply this model to our textual data. To this end, we assign `en_core_web_sm.load()` to an object called `nlp`, so that we can call it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the language model to the novel we worked with earlier, and check the amount of named entities it recognizes in the text. We can get our entities by calling the 'ents' from our Doc object [(see spaCy documentation here)](https://spacy.io/api/doc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myDoc = nlp(myBook)\n",
    "len(myDoc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how many named entities are in our novel, let's see what is considered a 'named entity' in the first place. The script below counts the 20 most common entities in our novel and saves them to a variable called top_entities.\n",
    "\n",
    "> **Exercise:** Print the top_entities in a dataframe with three columns: entity, type and count. Hint: use the pd.DataFrame() function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and count named entities\n",
    "top_entities = Counter((x.text, x.label_) for x in myDoc.ents).most_common(20)\n",
    "\n",
    "# Split the values into separate entities\n",
    "top_entities = [(entity, label, count) for (entity, label), count in top_entities]\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, named entities are **real-world entities**. In other words: they are things, places, people, organizations, or other entities that are the subjects in the text. Here's the full list of entity types that spaCy can recognize:\n",
    "\n",
    "- PERSON: People, including fictional.\n",
    "- NORP: Nationalities or religious or political groups.\n",
    "- FAC: Buildings, airports, highways, bridges, etc.\n",
    "- ORG: Companies, agencies, institutions, etc.\n",
    "- GPE: Countries, cities, states.\n",
    "- LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "- PRODUCT: Objects, vehicles, foods, etc. (not services).\n",
    "- EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LAW: Named documents made into laws.\n",
    "- LANGUAGE: Any named language.\n",
    "- DATE: Absolute or relative dates or periods.\n",
    "- TIME: Times smaller than a day.\n",
    "- PERCENT: Percentage, including \"%\".\n",
    "- MONEY: Monetary values, including unit.\n",
    "- QUANTITY: Measurements, as of weight or distance.\n",
    "- ORDINAL: \"first\", \"second\", etc.\n",
    "- CARDINAL: Numerals that do not fall under another type.\n",
    "\n",
    "Studying named entities is incredibly useful for understanding the most relevant actors in a corpus. Because NER analysis differentiates between different types of entities, it becomes possible to filter for specific classes. For example, the code below only keeps entities classified as 'People':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract named entities and filter for people (PERSON)\n",
    "people = [ent.text for ent in myDoc.ents if ent.label_ in ['PERSON']]\n",
    "top_10_people = Counter(people).most_common(10)\n",
    "\n",
    "# Count occurrences of people and show the 10 most common\n",
    "pd.DataFrame(top_10_people, columns=['Entity', 'Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Create a dataframe containing the top 20 entities that are classified as location or geopolitical entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### displacy\n",
    "\n",
    "``spacy`` includes an in-built visualization tool called ``displacy``. This tool can be used to visualize certain dimensions from ``spacy``, including the entities from our NER analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "displacy.render(myDoc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``displacy`` is also a handy tool to see how ``spacy`` operates under the hood. For example, it can show us how it interprets the dependencies between words in our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySentence = nlp(\"I gave three apples to my mother, but she refused them. She said she's allergic to them.\")\n",
    "\n",
    "displacy.render(mySentence, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "In this tutorial, we took our first steps in Natural Language Processing. Of course, we just scratched the surface here. There's tons more possibilities. Here are a few suggestions to try out:\n",
    "\n",
    "- [Sentiment analysis using NLTK](https://www.datacamp.com/tutorial/text-analytics-beginners-nltk)\n",
    "- [Gender identification by name](https://www.geeksforgeeks.org/python-gender-identification-by-name-using-nltk/)\n",
    "- [Analyzing semantic word similarity](https://www.geeksforgeeks.org/python-word-similarity-using-spacy/)\n",
    "\n",
    "There are many materials out there. Look around online for more ideas!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
